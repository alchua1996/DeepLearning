{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion_MNIST.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMu47vUmNruRHhvAIMPcyIy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alchua1996/DeepLearning/blob/master/Fashion_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWlIhZKZimwn"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader,SubsetRandomSampler\r\n",
        "from torchvision.datasets import FashionMNIST\r\n",
        "from torchvision import transforms as tfs\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.image as mpimg\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iQ5pXcokQ_7"
      },
      "source": [
        "def L(r,j): #low pass\r\n",
        "    if(2**j * r <= np.pi/2):\r\n",
        "        out = 1\r\n",
        "    elif(np.pi/2 <= 2**j * r <= np.pi):\r\n",
        "        out = np.cos(np.pi/2 * np.log2(2* 2**(j) * r/np.pi))\r\n",
        "    else:\r\n",
        "        out = 0\r\n",
        "    return out\r\n",
        "\r\n",
        "def H(r,j): #high pass\r\n",
        "    if(2**j * r <= np.pi/2):\r\n",
        "        out = 0\r\n",
        "    elif(np.pi/2 <= 2**j * r <= np.pi/4):\r\n",
        "        out = np.cos(np.pi/2 * np.log2(2**(j) * r/np.pi))\r\n",
        "    else:\r\n",
        "        out = 1\r\n",
        "    return out\r\n",
        "\r\n",
        "def G_q(theta, Q,q): #directional cones\r\n",
        "    alpha = 2**(Q - 1) * np.math.factorial(Q-1) / np.sqrt(Q*np.math.factorial(2*(Q-1)))\r\n",
        "    r1 = theta - np.pi * q/Q\r\n",
        "    if(r1 < -np.pi):\r\n",
        "        r1 += 2*np.pi\r\n",
        "    r2 = theta - np.pi * (q - Q)/Q\r\n",
        "    if(r2 > np.pi):\r\n",
        "        r2 -= 2*np.pi\r\n",
        "    if(np.abs(r1) <= np.pi/2 and np.abs(r2) <= np.pi/2):\r\n",
        "        out = np.cos(r1)**(Q-1) + np.cos(r2)**(Q-1)\r\n",
        "    elif(np.abs(r1) <= np.pi/2 and np.abs(r2) > np.pi/2):\r\n",
        "        out = np.cos(r1)**(Q-1)\r\n",
        "    elif(np.abs(r1) > np.pi/2 and np.abs(r2) <= np.pi/2):\r\n",
        "        out = np.cos(r2)**(Q-1)\r\n",
        "    else:\r\n",
        "        out = 0\r\n",
        "    return alpha * out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LInuPxpkVSe"
      },
      "source": [
        "def polar_grid(sample_rate_x, sample_rate_y): \r\n",
        "    grid = np.zeros((sample_rate_x,sample_rate_y,2), dtype = 'float')\r\n",
        "    x = np.linspace(-np.pi, np.pi, num=sample_rate_x, endpoint=False)\r\n",
        "    y = np.linspace(-np.pi, np.pi, num=sample_rate_y, endpoint=False)\r\n",
        "    xx, yy = np.meshgrid(x, y, sparse=False, indexing='ij')\r\n",
        "    rr = np.sqrt(xx**2 + yy**2)\r\n",
        "    theta = np.arctan2(yy,xx)\r\n",
        "    grid[:,:,0] = rr\r\n",
        "    grid[:,:,1] = theta\r\n",
        "    return grid "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzuCQcdUkYdu"
      },
      "source": [
        "def L_grid(grid, J=0):\r\n",
        "    N = grid.shape[0]\r\n",
        "    M = grid.shape[1]\r\n",
        "    L_matrix = np.zeros((N,M), dtype = 'float')\r\n",
        "    for n in range(N):\r\n",
        "        for m in range(M):\r\n",
        "            L_matrix[n,m] = L(grid[n,m,0],J)\r\n",
        "    return L_matrix\r\n",
        "\r\n",
        "def H_grid(grid, j=0):\r\n",
        "    N = grid.shape[0]\r\n",
        "    M = grid.shape[1]\r\n",
        "    H_matrix = np.zeros((N,M))\r\n",
        "    for n in range(N):\r\n",
        "        for m in range(M):\r\n",
        "            H_matrix[n,m] = H(grid[n,m,0],j)\r\n",
        "    return H_matrix\r\n",
        "\r\n",
        "def G_grid(grid,Q):\r\n",
        "    N = grid.shape[0]\r\n",
        "    M = grid.shape[1]\r\n",
        "    G_matrix = np.zeros((N,M,Q))\r\n",
        "    for n in range(N):\r\n",
        "        for m in range(M):\r\n",
        "            for q in range(Q):\r\n",
        "                G_matrix[n,m,q] = G_q(grid[n,m,1], Q, q)\r\n",
        "    return G_matrix            \r\n",
        "\r\n",
        "def psi_grid(grid, Q, J):\r\n",
        "    N = grid.shape[0]\r\n",
        "    M = grid.shape[1]\r\n",
        "    psi_matrix = np.zeros((N,M,Q,J))\r\n",
        "    G = G_grid(grid,Q)\r\n",
        "    for j in range(J):\r\n",
        "        L = L_grid(grid, j)\r\n",
        "        H = H_grid(grid, j+1)\r\n",
        "        for q in range(Q):\r\n",
        "            psi_matrix[:,:,q,j] = G[:,:,q] * L * H     \r\n",
        "    return psi_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWvwnHuFkclr"
      },
      "source": [
        "def conv_frequency(img, filter):\r\n",
        "    convolved_img = np.fft.ifft2(np.fft.fft2(img) * np.fft.fftshift(filter))\r\n",
        "    return convolved_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poXIa8QSkezk"
      },
      "source": [
        "def get_next_layer(coeffs,filters,num_angles, d):\r\n",
        "    #current number of wavelet coefficients in coeffs array\r\n",
        "    num_coeffs = num_angles**(d-1)\r\n",
        "    N = coeffs.shape[0]\r\n",
        "    count = 0\r\n",
        "    if(d == 1):\r\n",
        "        next_layer = np.zeros((N,N, num_angles), dtype = 'complex')\r\n",
        "        for m in range(num_angles):\r\n",
        "            next_layer[:,:,count] = conv_frequency(coeffs, filters[:,:,m])\r\n",
        "            count += 1 \r\n",
        "    else:\r\n",
        "        next_layer = np.zeros((N,N, num_coeffs * num_angles), dtype = 'complex')\r\n",
        "        count = 0\r\n",
        "        for n in range(num_coeffs):\r\n",
        "            for m in range(num_angles):\r\n",
        "                next_layer[:,:,count] = conv_frequency(coeffs[:,:,n], filters[:,:,m])\r\n",
        "                count += 1 \r\n",
        "    return next_layer"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGCflUpfkgqh"
      },
      "source": [
        "def wavelet_scattering_transform(img, psi, phi, num_angles, depth):\r\n",
        "    #number of wavelet coefficients\r\n",
        "    num_coeff = 0\r\n",
        "    for n in range(depth+1):\r\n",
        "        num_coeff += num_angles**n\r\n",
        "\r\n",
        "    #wavelet coefficient index\r\n",
        "    count = 0\r\n",
        "\r\n",
        "    #initialize array to store all wavelet coefficients\r\n",
        "    wavelet_coefficients = np.zeros((img.shape[0],img.shape[0],num_coeff), dtype = 'complex')\r\n",
        "\r\n",
        "    for d in range(depth+1):\r\n",
        "        if d == 0:\r\n",
        "            curr_layer = img \r\n",
        "            wavelet_coefficients[:,:,count] = conv_frequency(curr_layer, phi)\r\n",
        "            count+=1\r\n",
        "        else: \r\n",
        "            next_layer = get_next_layer(curr_layer, psi[:,:,:,d-1], num_angles, d)\r\n",
        "            next_layer = np.abs(next_layer)\r\n",
        "            for n in range(num_angles**d):\r\n",
        "                wavelet_coefficients[:,:,count] = conv_frequency(next_layer[:,:,n],phi)\r\n",
        "                count += 1 \r\n",
        "            curr_layer = next_layer        \r\n",
        "    return wavelet_coefficients"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVpOsBMk6dVW"
      },
      "source": [
        "def downsample(x):\r\n",
        "    x_down = np.zeros(int(len(x)/2))\r\n",
        "    for n in range(len(x)):\r\n",
        "        if (n % 2 == 0):\r\n",
        "            x_down[int(n/2)] = x[n]\r\n",
        "    return x_down"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2gYrOt3khI2"
      },
      "source": [
        "depth  = 2\r\n",
        "num_angles = 4\r\n",
        "grid = polar_grid(28, 28)\r\n",
        "psi = psi_grid(grid, num_angles, depth)\r\n",
        "phi = L_grid(grid, depth)\r\n",
        "\r\n",
        "num_coeff = 0\r\n",
        "for n in range(depth+1):\r\n",
        "    num_coeff += num_angles**n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfnq6X0Pi2YZ"
      },
      "source": [
        "TRAIN_BATCH = 100\r\n",
        "VAL_BATCH = 100\r\n",
        "TEST_BATCH = 1\r\n",
        "\r\n",
        "# Transform data to PIL images\r\n",
        "transforms = tfs.Compose([tfs.ToTensor()])\r\n",
        "\r\n",
        "# Train/Val Subsets\r\n",
        "train_mask = range(50000)\r\n",
        "val_mask = range(50000, 60000)\r\n",
        "\r\n",
        "# Download/Load Dataset\r\n",
        "train_dataset = FashionMNIST('./data', train=True, transform=transforms, download=True)\r\n",
        "test_dataset = FashionMNIST('./data', train=False, transform=transforms, download=True)\r\n",
        "\r\n",
        "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\r\n",
        "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\r\n",
        "\r\n",
        "train_feat = next(iter(train_loader))[0].numpy()\r\n",
        "train_feat.reshape((len(train_dataset),28,28))\r\n",
        "test_feat = next(iter(test_loader))[0].numpy()\r\n",
        "test_feat.reshape((len(test_dataset),28,28))\r\n",
        "\r\n",
        "train_labels = next(iter(train_loader))[1]\r\n",
        "test_labels = next(iter(test_loader))[1]"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iCLCFweRoSk"
      },
      "source": [
        "train_transform = np.zeros((train_feat.shape[0], int(28*28*num_coeff/2)))\r\n",
        "for n in range(train_feat.shape[0]):\r\n",
        "    sample = train_feat[n,:,:].reshape((28,28))\r\n",
        "    trans_sample = np.real(wavelet_scattering_transform(sample, psi, phi, num_angles, depth))\r\n",
        "    trans_sample = np.reshape(trans_sample,28*28*num_coeff)\r\n",
        "    downsample_x = downsample(trans_sample)\r\n",
        "    train_transform[n,:] = downsample_x \r\n",
        "\r\n",
        "test_transform = np.zeros((test_feat.shape[0], int(28*28*num_coeff/2)))\r\n",
        "for n in range(test_feat.shape[0]):\r\n",
        "    sample = test_feat[n,:,:].reshape((28,28))\r\n",
        "    trans_sample = np.real(wavelet_scattering_transform(sample, psi, phi, num_angles, depth))\r\n",
        "    trans_sample = np.reshape(trans_sample,28*28*num_coeff)\r\n",
        "    downsample_x = downsample(trans_sample)\r\n",
        "    test_transform[n,:] = downsample_x \r\n",
        "\r\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeE20UePFC6e"
      },
      "source": [
        "# from sklearn.decomposition import PCA\r\n",
        "# train_pca = PCA(train_transform)\r\n",
        "# test_pca = train_pca *\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HNLZKAoRpuM"
      },
      "source": [
        "train_transform_T = torch.Tensor(train_transform)\r\n",
        "test_transform_T = torch.Tensor(test_transform)\r\n",
        "train_data = TensorDataset(train_transform_T, train_labels)\r\n",
        "test_data = TensorDataset(test_transform_T, test_labels)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VFt9IgDOkre"
      },
      "source": [
        "# Data Loaders\r\n",
        "train_load = DataLoader(train_data, batch_size=TRAIN_BATCH, sampler=SubsetRandomSampler(train_mask))\r\n",
        "val_load = DataLoader(train_data, batch_size=VAL_BATCH, sampler=SubsetRandomSampler(val_mask))\r\n",
        "test_load = DataLoader(test_data, batch_size=TEST_BATCH)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLGo9yexjDcT"
      },
      "source": [
        "class ANN(nn.Module):\r\n",
        "    def __init__(self, nin, nout):\r\n",
        "        super(ANN, self).__init__()\r\n",
        "        self.dropout = nn.Dropout(0.2)\r\n",
        "        self.Linear1 = nn.Linear(nin, 100)\r\n",
        "        self.Linear2 = nn.Linear(100, 25)\r\n",
        "        self.Linear3 = nn.Linear(25, nout)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = F.relu(self.Linear1(x))\r\n",
        "        x = self.dropout(x)\r\n",
        "        x = F.relu(self.Linear2(x))\r\n",
        "        x = self.dropout(x)\r\n",
        "        x = F.relu(self.Linear3(x))\r\n",
        "        return F.softmax(x, dim = -1)\r\n",
        "    \r\n",
        "    def predict(self, x):\r\n",
        "        x = F.relu(self.Linear1(x))\r\n",
        "        x = F.relu(self.Linear2(x))\r\n",
        "        x = F.relu(self.Linear3(x))\r\n",
        "        return F.softmax(x, dim = -1)\r\n",
        "\r\n",
        "def weights_init(m):\r\n",
        "    if isinstance(m, nn.Linear):\r\n",
        "        torch.nn.init.xavier_uniform(m.weight.data)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AoV492rjMVD"
      },
      "source": [
        "def train_eval(epoch):\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    loss_sum = 0\r\n",
        "    for data, labels in train_load:\r\n",
        "        outputs = model(data)\r\n",
        "        predicted = torch.argmax(outputs.data,-1)\r\n",
        "        total += labels.size(0)\r\n",
        "        correct += (predicted.float() == labels.float()).sum()\r\n",
        "        loss_sum += loss_function(outputs,labels)\r\n",
        "        \r\n",
        "    if (epoch+1) % 1 == 0:\r\n",
        "        print(\"Epoch {}:\".format(epoch+1))\r\n",
        "        print('Train accuracy: %f %%' % (100.0 * correct / total))\r\n",
        "        print('Train loss: %f' % (loss_sum.data.numpy().item() / total))\r\n",
        "\r\n",
        "    #return 100.0 * correct / total, loss_sum.data.numpy().item() / total\r\n",
        "    return\r\n",
        "    \r\n",
        "def val_eval(epoch):\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    loss_sum = 0\r\n",
        "    for data, labels in val_load:\r\n",
        "        outputs = model.predict(data)\r\n",
        "        predicted = torch.argmax(outputs.data,-1)\r\n",
        "        total += labels.size(0)\r\n",
        "        correct += (predicted.float() == labels.float()).sum()\r\n",
        "        loss_sum += loss_function(outputs,labels)\r\n",
        "\r\n",
        "    if (epoch+1) % 1 == 0:\r\n",
        "        print('Validation accuracy: %f %%' % (100.0 * correct / total))\r\n",
        "        print('Validation loss: %f' % (loss_sum.data.numpy().item() / total))\r\n",
        "\r\n",
        "    return "
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r48Qer4Uk03Y",
        "outputId": "5553ee31-4978-4879-e4e3-1d13e5ab6d68"
      },
      "source": [
        "model = ANN(int(28*28*num_coeff/2), 10)\r\n",
        "weights_init(model)\r\n",
        "\r\n",
        "loss_function = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\r\n",
        "\r\n",
        "for epoch in range(100):\r\n",
        "    tot_loss = 0.0\r\n",
        "    for i, (features, labels) in enumerate(train_load):\r\n",
        "        optimizer.zero_grad() \r\n",
        "        output = model(features)\r\n",
        "        loss = loss_function(output, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "    tot_loss += loss.item()\r\n",
        "    train_eval(epoch)\r\n",
        "    val_eval(epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1:\n",
            "Train accuracy: 71.167999 %\n",
            "Train loss: 0.017560\n",
            "Validation accuracy: 72.290001 %\n",
            "Validation loss: 0.017391\n",
            "Epoch 2:\n",
            "Train accuracy: 79.162003 %\n",
            "Train loss: 0.016753\n",
            "Validation accuracy: 80.360001 %\n",
            "Validation loss: 0.016585\n",
            "Epoch 3:\n",
            "Train accuracy: 80.578003 %\n",
            "Train loss: 0.016593\n",
            "Validation accuracy: 81.540001 %\n",
            "Validation loss: 0.016467\n",
            "Epoch 4:\n",
            "Train accuracy: 81.253998 %\n",
            "Train loss: 0.016517\n",
            "Validation accuracy: 82.080002 %\n",
            "Validation loss: 0.016410\n",
            "Epoch 5:\n",
            "Train accuracy: 81.472000 %\n",
            "Train loss: 0.016487\n",
            "Validation accuracy: 82.230003 %\n",
            "Validation loss: 0.016395\n",
            "Epoch 6:\n",
            "Train accuracy: 82.012001 %\n",
            "Train loss: 0.016420\n",
            "Validation accuracy: 82.550003 %\n",
            "Validation loss: 0.016331\n",
            "Epoch 7:\n",
            "Train accuracy: 83.047997 %\n",
            "Train loss: 0.016329\n",
            "Validation accuracy: 83.660004 %\n",
            "Validation loss: 0.016241\n",
            "Epoch 8:\n",
            "Train accuracy: 84.150002 %\n",
            "Train loss: 0.016211\n",
            "Validation accuracy: 84.750000 %\n",
            "Validation loss: 0.016132\n",
            "Epoch 9:\n",
            "Train accuracy: 85.323997 %\n",
            "Train loss: 0.016103\n",
            "Validation accuracy: 85.639999 %\n",
            "Validation loss: 0.016039\n",
            "Epoch 10:\n",
            "Train accuracy: 85.398003 %\n",
            "Train loss: 0.016087\n",
            "Validation accuracy: 85.940002 %\n",
            "Validation loss: 0.016027\n",
            "Epoch 11:\n",
            "Train accuracy: 85.452003 %\n",
            "Train loss: 0.016087\n",
            "Validation accuracy: 85.580002 %\n",
            "Validation loss: 0.016052\n",
            "Epoch 12:\n",
            "Train accuracy: 86.388000 %\n",
            "Train loss: 0.015992\n",
            "Validation accuracy: 86.480003 %\n",
            "Validation loss: 0.015961\n",
            "Epoch 13:\n",
            "Train accuracy: 86.680000 %\n",
            "Train loss: 0.015963\n",
            "Validation accuracy: 86.830002 %\n",
            "Validation loss: 0.015929\n",
            "Epoch 14:\n",
            "Train accuracy: 86.695999 %\n",
            "Train loss: 0.015957\n",
            "Validation accuracy: 86.919998 %\n",
            "Validation loss: 0.015916\n",
            "Epoch 15:\n",
            "Train accuracy: 86.676003 %\n",
            "Train loss: 0.015960\n",
            "Validation accuracy: 86.639999 %\n",
            "Validation loss: 0.015936\n",
            "Epoch 16:\n",
            "Train accuracy: 86.875999 %\n",
            "Train loss: 0.015931\n",
            "Validation accuracy: 87.000000 %\n",
            "Validation loss: 0.015917\n",
            "Epoch 17:\n",
            "Train accuracy: 87.564003 %\n",
            "Train loss: 0.015867\n",
            "Validation accuracy: 87.470001 %\n",
            "Validation loss: 0.015858\n",
            "Epoch 18:\n",
            "Train accuracy: 86.795998 %\n",
            "Train loss: 0.015953\n",
            "Validation accuracy: 86.629997 %\n",
            "Validation loss: 0.015952\n",
            "Epoch 19:\n",
            "Train accuracy: 86.966003 %\n",
            "Train loss: 0.015922\n",
            "Validation accuracy: 86.930000 %\n",
            "Validation loss: 0.015926\n",
            "Epoch 20:\n",
            "Train accuracy: 87.856003 %\n",
            "Train loss: 0.015841\n",
            "Validation accuracy: 87.449997 %\n",
            "Validation loss: 0.015854\n",
            "Epoch 21:\n",
            "Train accuracy: 87.047997 %\n",
            "Train loss: 0.015921\n",
            "Validation accuracy: 86.860001 %\n",
            "Validation loss: 0.015922\n",
            "Epoch 22:\n",
            "Train accuracy: 87.512001 %\n",
            "Train loss: 0.015869\n",
            "Validation accuracy: 87.459999 %\n",
            "Validation loss: 0.015858\n",
            "Epoch 23:\n",
            "Train accuracy: 87.837997 %\n",
            "Train loss: 0.015832\n",
            "Validation accuracy: 87.559998 %\n",
            "Validation loss: 0.015851\n",
            "Epoch 24:\n",
            "Train accuracy: 88.103996 %\n",
            "Train loss: 0.015805\n",
            "Validation accuracy: 87.949997 %\n",
            "Validation loss: 0.015815\n",
            "Epoch 25:\n",
            "Train accuracy: 88.236000 %\n",
            "Train loss: 0.015790\n",
            "Validation accuracy: 88.059998 %\n",
            "Validation loss: 0.015798\n",
            "Epoch 26:\n",
            "Train accuracy: 87.452003 %\n",
            "Train loss: 0.015875\n",
            "Validation accuracy: 87.480003 %\n",
            "Validation loss: 0.015854\n",
            "Epoch 27:\n",
            "Train accuracy: 87.912003 %\n",
            "Train loss: 0.015823\n",
            "Validation accuracy: 88.080002 %\n",
            "Validation loss: 0.015812\n",
            "Epoch 28:\n",
            "Train accuracy: 88.143997 %\n",
            "Train loss: 0.015804\n",
            "Validation accuracy: 88.209999 %\n",
            "Validation loss: 0.015792\n",
            "Epoch 29:\n",
            "Train accuracy: 88.559998 %\n",
            "Train loss: 0.015761\n",
            "Validation accuracy: 88.269997 %\n",
            "Validation loss: 0.015781\n",
            "Epoch 30:\n",
            "Train accuracy: 87.737999 %\n",
            "Train loss: 0.015839\n",
            "Validation accuracy: 87.980003 %\n",
            "Validation loss: 0.015815\n",
            "Epoch 31:\n",
            "Train accuracy: 88.276001 %\n",
            "Train loss: 0.015789\n",
            "Validation accuracy: 88.089996 %\n",
            "Validation loss: 0.015797\n",
            "Epoch 32:\n",
            "Train accuracy: 88.258003 %\n",
            "Train loss: 0.015787\n",
            "Validation accuracy: 88.160004 %\n",
            "Validation loss: 0.015801\n",
            "Epoch 33:\n",
            "Train accuracy: 88.300003 %\n",
            "Train loss: 0.015787\n",
            "Validation accuracy: 87.639999 %\n",
            "Validation loss: 0.015842\n",
            "Epoch 34:\n",
            "Train accuracy: 88.884003 %\n",
            "Train loss: 0.015725\n",
            "Validation accuracy: 88.839996 %\n",
            "Validation loss: 0.015730\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYcR_Cwtc7dh",
        "outputId": "9aeed24e-1a1c-4f36-c7f0-a517f4be45b5"
      },
      "source": [
        "correct = 0\r\n",
        "total = 0\r\n",
        "for data, labels in test_load:\r\n",
        "    outputs = model.predict(data)\r\n",
        "    predicted = torch.argmax(outputs.data,-1)\r\n",
        "    total += labels.size(0)\r\n",
        "    correct += (predicted.float() == labels.float()).sum()\r\n",
        "print('Test accuracy: %f %%' % (100.0 * correct / total))\r\n",
        "\r\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 88.139999 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}